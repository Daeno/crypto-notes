\course{Complexity Reading Group}{"How to Simulate It" Notes}{Kai-Ming, Chung}{Yin-Hsun, Huang}
\vspace*{4mm} 

\section{Introduction}
Simulation is a way of comparig what happens in the "real world" to what happens in an "ideal world" where the primitive in question is {\it secure by definition}. We start our discussion from semantic security of an private-key encryption. And move on to two-party computation. The semi-honest adversary model is relativly easy and is discussed in Section 4. The rest of article focus on the malicious adversary model.

\vspace{4mm}
\begin{tabular}{|l|c|c|}
\hline
Functionality & Input & Output\\
\hline
(Sec. 5) Zero Knowledge & X & X\\
(Sec. 6) Coin Tossing & X & O \\
(Sec. 7) Oblivious Transfer & O & O \\
\hline
\end{tabular}

\section{Preliminaries and Notation}
\YHnote{Lindel made some discussion on the order of quantifiers for computational indistinguishability. I should elaborate it.}

\section{Semantic Security}
We begin by recalling the definition of semantic security.
\begin{definition}[Semantic Security] A {\it private-key encryption scheme} $(G,E,D)$ is {\bf semantically secure} (in the private-key model) if for every non-uniform probabilistic-polynomial time algorithm $\calA$, there exists a non-uniform probabilistic-polynomial time algorithm $\calA'$ such that for every probability ensemble $\{X_n\}_{n\in\N}$ with $\abs{X_n}\leq poly(n)$, every pair of polynomially-bounded functions $f,h:\{0,1\}^*\to\{0,1\}^*$, every positive polynomial $p(\cdot)$ and all sufficiently large $n$:
\begin{align*}
    \Pr[k\leftarrow G(1^n);\calA(1^n,E_k(X_n), h(1^n,X_n)) = f(1^n, X_n)] \\
    < \Pr[\calA'(1^n,1^{\abs{X_n}}, h(1^n,X_n)) = f(1^n,X_n) ]+ \frac{1}{p(n)}
\end{align*}
with probability over $X_n$ and the randomness of $(G,E)$ and $\calA$ or$\calA'$.
\end{definition}
Though the definition of {\it semantic security} is not directly related to simulation-based paradigm. In particular, $\calA'$ simulate the execution of $\calA$ as follows:
\begin{description}
    \item [\bf Simulator $\calA'$:] Upon input $1^n,h=h(1^n,X_n)$, algorithm $\calA'$ works as follows
    \begin{enumerate}
        \item $\calA'$ runs $G(1^n)$ in order to receive $k$.
        \item $\calA'$ computes $c=E_k(0^{\abs{X_n}})$
        \item $\calA'$ outputs $\calA(1^n,c,a^{\abs{X_n}},h)$
    \end{enumerate}
\end{description}
If encryptions are {\it indistinguishable}, then the simulation process should output the same as $\calA$ with approximately the same probability.
\section{Secure Computation - Semi-Honest Adversaries}
\subsection{Background}
The {\it semi-honest adversaries} in two-party computation is an adversary that follows the protocol but wants to learn information from the transcript of messages. Hence the nickname, honest-but-curious adversaries. 
\begin{remark}
    This is a very weak adversary model. There are no guarantees to the security against adversaries that deviate from the protocol.
\end{remark}

\begin{definition}[Two-party computation] A two-party random process maps pairs of inputs to pairs of outputs. We refer to such a process as a {\bf functionality} and denote it $f:\{0,1\}^*\times\{0,1\}^*\to\{0,1\}^*\times\{0,1\}^*$, where $f=(f_1,f_2)$. For every pair of inputs $x,y\in\{0,1\}^n$, the pair of output is a random variable $(f_1(x,y),f_2(x,y))$. The first (resp. second) party (with input $x$ (resp. $y$)) wishes to obtain $f_1(x,y)$ (resp. $f_2(x,y)$).
\end{definition}
The goal of simulator is to generate the view only from a party's input and output. The security here is formalized by saying that a party's view in a protocol execution be simulatable given only its input and output. 
\begin{description}
\item[Definition of security.] We begin with the following notation:
\begin{itemize}
    \item Let $f=(f_1,f_2)$ be a PPT functionality and let $\pi$ be a two-party protocol for computing $f$.
    \item The view of the $i$th, where $i=1$ (resp. $i=2$), party is ${\sf view}^\pi_i(x,y,n)$ and equals $(w,r^i;m^i_1,\cdots,m^i_t)$, where $w=x$ (resp. $w=y$). $r^i$ is the internal randomness of $i$th party and $m^i_\ell$ is the $\ell$th message that $i$th party received.
    \item The output of the $i$th party is denoted by $\out^\pi_i(x,y,n)$. We denote the joint output of both parties by $\out^\pi(x,y,n)=(\out^\pi_1(x,y,n))(\out^\pi_2(x,y,n))$.
\end{itemize}
\end{description}
\begin{definition}Let $f=(f_1,f_2)$ be a functionality. We say that {\sf$\pi$ securely computes $f$ in the presence of static semi-honest adversarits} if there exists PPT algorithm $\calS_1$ and $\calS_2$ such that
\begin{align*}
    \{(\calS_1(1^n,x,f_1(x,y)), f(x,y))\}_{x,y,n} \stackrel{c}{\equiv} \{(\view^\pi_1(x,y,n),\out^\pi(x,y,n))\}_{x,y,n} \\
    \{(\calS_2(1^n,x,f_2(x,y)), f(x,y))\}_{x,y,n} \stackrel{c}{\equiv} \{(\view^\pi_2(x,y,n),\out^\pi(x,y,n))\}_{x,y,n}
\end{align*}
where $x,y\in\{0,1\}^*$ such that $\abs{x} = \abs{y}$, and $n\in\N$.
\end{definition}
Observe that for probabilistic functionalities, the simulator needs to generate a joint distribution of $(\view_i^\pi(x,y), \out^\pi(x,y))$. We can obtain a simpler security formulation for deterministic functionalities:
\begin{description}
\item[Correctness,] meaning that the output of the parties is correct. Formally, it requires that there exists a negligible function $\mu$ such that for every $x,y\in\{0,1\}^*$ and every $n$
    $$\Pr[\out^\pi(x,y,n)\neq f(x,y,n)]\leq\mu(n)$$
\item[Privacy] requires that there exists PPT $\calS_1$ and $\calS_2$ such that
\begin{align*}
    \{\calS_1(1^n,x,f_1(x,y))\}_{x,y,n} \stackrel{c}{\equiv} \{\view^\pi_1(x,y,n)\}_{x,y,n} \\
    \{\calS_2(1^n,y,f_2(x,y))\}_{x,y,n} \stackrel{c}{\equiv} \{\view^\pi_2(x,y,n)\}_{x,y,n} \\
\end{align*}
\end{description}
\begin{remark} There are many problemss that become trivial in the case of semi-honest adversaries. Such problems include zero knowledge, commitment and coin-tossing.
\end{remark}
\subsection{Oblivious Transfer for Semi-Honest Adversaries}
The bit oblivious transfer functionality is defined by $f((b_0,b_1), \sigma )=(\lambda,b_\sigma)$, where $b_0,b_1\in\{0,1\}$ and $\lambda$ denotes empty string.
\subsubsection{Background and Tools}
\begin{definition}[Trapdoor Permutations]
Trapdoor permutations is a collection of functions $\{f_\alpha\}_\alpha$ accompanied by four PPT algorithms $I,S,F,F^{-1}$ such that:
\begin{enumerate}
\item $I(1^n)$ select a random $n$-bit index $\alpha$ of a premutation $f_\alpha$ and a corresponding trapdoor $\tau$. Let $I_1(1^n)\equiv\alpha$.
\item $S(\alpha;r)$ samples an (almost uniform) element in the domain of $f_\alpha$. We assume $r\in\{0,1\}^n$.
\item $F(\alpha,x)=f_\alpha(x)$
\item $F^{-1}(\tau,y)=f^{-1}_\alpha(y)$
\end{enumerate}
\end{definition}
\YHnote{Need to check the security definition of T.P.}

\begin{definition}[Enhanced Trapdoor Permutations] A collection of {\sf trapdoor permutations} is a collection of {\sf enhanced trapdoor permutations} if for every non-uniform PPT adversary $\calA$ there exists a negligible function $\mu(\cdot)$ such that for every $n$,
$$\Pr[\calA(1^n,\alpha,r)=f^{-1}(S(\alpha;r))]\leq\mu(n)$$
where $\alpha\leftarrow I_1(1^n)$ and $r\stackrel{R}{\leftarrow}\{0,1\}^n$.
\end{definition}
\begin{definition}[Hard-Core Predicate] We say that $B$ is a {\sf hard-core predicate} of $(I,S,F,F^{-1})$ if for every non-uniform PPT adversary $\calA$ there exists a negligible function $\mu(\cdot)$ such that for every $n$,
$$\Pr[\calA(1^n,\alpha,r)=B(\alpha, f^{-1}(S(\alpha;r)))]\leq\frac{1}{2}+\mu(n)$$
\end{definition}
\begin{adjustbox}{minipage=\linewidth,fbox}
\begin{protocol}[Bit Oblivious Transfer]$ $
    \begin{description}
        \item[Inputs.] $P_1$ has $(b_0,b_1)$ and $P_2$ has $\sigma\in\{0,1\}$. Both have $(I,S,F,F^{-1})$ and corresponding hard-core predicate $B$.
        \item[The Protocol.] $ $\newline\newline
        \begin{adjustbox}{minipage=0.6\linewidth}
            \begin{tabular}{lcc}
            $P_1((b_0,b_1);r_1)$ & & $P_2(\sigma;(r_0^2,r_1^2))$ \\
            \\
            $(\alpha,\tau)\gets I(1^n;r_1)$ & \rextlinearrow{\alpha}{24} & \\
            & & $x_\sigma\gets S(\alpha;r_\sigma^2)$\\
            & & $y_{1-\sigma}\gets S(\alpha;r_{1-\sigma}^2)$\\
            & & compute $y_\sigma = F(\alpha,x_\sigma) $\\
            & \lextlinearrow{y_0,y_1}{24} & \\
            compute $x_0=F^{-1}(\tau,y_0)$ & & \\
            $x_1=F^{-1}(\tau,y_1)$ & & \\
            and \\ \\
            $\beta_0=B(\alpha,x_0)\oplus b_0$ & \rextlinearrow{\beta_0,\beta_1}{24}\\
            $\beta_1=B(\alpha,x_1)\oplus b_1$ & & compute $b_\sigma=B(\alpha,x_\sigma)\oplus\beta_\sigma$ \\
            \end{tabular}
        \end{adjustbox}
    \end{description}
\end{protocol}
\end{adjustbox}
\vspace{2mm}

\begin{algorithm}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
$S_1(1^n,(b_0,b_1);r,r_0,r_1)$\;
\Input{The input of $P_1$ and the ideal functionality output $f_1(\cdot,\cdot)$.}
\Output{A simulated view close to real view of $P_1$.}
\caption{Simulator of corrupted $P_1$.} \label{alg:bot-p1-sim}
Initialize $P_1$ with random tape $r$\;
Computes $(\alpha,\tau)\gets I(1^n;r)$\;
Computes $y_0\gets S(\alpha;r_0), y_1\gets S(\alpha;r_1)$\;
Output $((b_0,b_1),r;(y_0,y_1))$\;
\end{algorithm}

\begin{algorithm}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
$S_2(1^n,\sigma,b_\sigma,;r,r_0,r_1)$\;
\Input{The input of $P_2$ and the ideal functionality output $f_2(\cdot,\cdot)$.}
\Output{A simulated view close to real view of $P_2$.}
Initialize $P_2$ with random tape $(r_0,r_1)$\;
Computes $(\alpha,\tau)\gets I(1^n;r)$\;
Computes $x_\sigma\gets S(\alpha;r_\sigma), y_{1-\sigma}\gets S(\alpha;r_{1-\sigma})$\;
Set $x_{1-\sigma}=F^{-1}(\tau,y_{1-\sigma})$\;
Compute $\beta_\sigma=B(\alpha,x_\sigma)\oplus b_\sigma$\;
$\beta_{1-\sigma}=B(\alpha,x_{1-\sigma})$\;
Output $(\sigma,r_0,r_1;\alpha,(\beta_0,\beta_1))$\;

\caption{Simulator of corrupted $P_2$.} \label{alg:bot-p2-sim}
\end{algorithm}

\begin{description}
\item[$P_1$ is currupted.] Construct simulator $S_1$ as Algorithm \ref{alg:bot-p1-sim}. \\
We argue that the output of $S_1$ is statistically close to the view of corrupted $P_1$ with random tape equals to $r$. The simulator has no information about $P_2$'s input $\sigma$, so it cannot compute $y_\sigma=F(\alpha,x_\sigma)$. However, the definition of trapdoor permutation states that the output of $S(\alpha)$ is almost uniform. Thus,
    $$\{(F(\alpha,x_0),y_1)\}\stackrel{s}{\equiv}\{(y_0,y_1)\}\stackrel{s}{\equiv}\{(y_0,F(\alpha,x_1))\}$$
where $x_0,x_1,y_0,y_1$ are sampled by $S(\alpha)$ with independent randomness. This implies
$$\{S_1(1^n,(b_0,b_1))\}\sequiv\{\view^\pi_1((b_0,b_1),\sigma)\}$$
\begin{remark}
Step 1. of the simulator is necessary since it binds the view of simulator with the real view of $P_1$.
\end{remark}
\item[$P_2$ is corrupted.] Construct simulator $S_2$ as Algorithm \ref{alg:bot-p2-sim}:

\end{description}

\section{Zero Knowledge}
\subsection{Notations}
In this section, the {\it security parameter} is set to be the length of common input, namely $n=\abs{x}$. Let $A$ and $B$ be a probabilistic polynomial-time machine. We denote by $\out_B(A(x,y,r_A),B(x,z,r_B))$ the output of party $B$ in an interactive execution with party $A$, on public input $x$, where $A$ has auxiliary-input $y$ and random-tape $r_A$, and $B$ has auxiliary input $z$ and random-tape $r_B$.
\begin{definition}[Interactive Proof System] {\it(Informal)} An interactive proof system for a language $L$ involves a prover $P$ and a verifier $V$. The prover $P$ tries to convince $V$ that the common input $x$ is in language $L$. There are two properties for such a proof system:
\begin{enumerate}
\item {\it Completeness:} Exists honest $P$ such that $\out_V(P(x,w,r_P),V(x,r_V))=1$ with overwhelmly high probability on input $x\in L$.
\item {\it Soundness:} For all (cheating) prover $P^*$, $\out_V(P^*(x,z,r_P),V(x,r_V))=1$ with at most negiligible probability on input $x\notin L$.
\end{enumerate}
\end{definition}

\begin{definition}[Black-box Computational Zero Knowledge] Let $(P,V)$ be an interactive proof system for an $\NP$-language $L$, and let $R_L$ be the $\NP$-relation. We say that $(P,V)$ is {\sf black-box computational zero knowledge} if there exists a PPT oracle machine $\simu$ such that for every PPT algorithm $V^*$, it holds that:
$$\{\out_{V^*}(P(x,w),V^*(x,z))\}_{(x,w)\in R_L,z\in\{0,1\}^*}\cequiv\{\simu^{V^*(x,z,r,\cdot)}(x)\}_{x\in L,z\in\{0,1\}^*}$$
where $r$ is uniformly distributed, and where $V^*(x,z,r,\cdot)$ denotes the next-message function of $V^*$ when the common input $x$, auxiliary input $z$ and random-tape $r$ are fixed.
\end{definition}

\begin{remark} Noted that we only care the distribution when $(x,w)\in R_L$. Why?
\end{remark}
\begin{remark} We fix the random tape of the $V^*$. In most case, this does not help since the adversary can ignore the random-tape and use a pseudorandom runction applied to its history with an internally hardcoded key.
\end{remark}

\subsection{Preliminaries - Commitment Schemes}
Let $\com$ be a non-interactive perfectly binding commitment scheme. Let $c=\com(x;r)$ denote a commitment to $x$ using randomness $r$ omitting the security parameter. Let $\com(x)$ denote a commitment using uniform randomness. Let $\decom(c)$ denote the decommitment value of $c$; if $c=\com(x;r)$ then $\decom(c)=(x,r)$.
There are two properties of a commitment scheme:
\begin{enumerate}
\item {\it (Computational) Hiding.} Informally, it states that the commitments to different strings are (computationally) indistinguishable. For bit commitmens, it means $\calC_0\cequiv\calC_1$ where $\calC_b\equiv\{\com(b;U_n)\}_{n\in\N}$.
\item {\it (Perfect) Binding.} All commitments of different values are disjoint; for all $x_1\neq x_2$ it holds that $\calC_{x_1}\cap\calC_{x_2}$ where $\calC_{x}=\{c|\exists r:c=\com(x;r)\}$.
\end{enumerate}
\begin{description}
\item[LR-security of commitments.] Define the oracle as $LR^b_{\com}(x_0,x_1)=\begin{cases}\com(x_b) & \text{if } \abs{x_0}=\abs{x_1},\\ \perp & \text{otherwise} \end{cases}$. The experiment is as follows:
\begin{description}
\item[Experiment ${\sf LR}$-commit$_{\com,\calA}(1^n)$:]$ $
\begin{enumerate}
\item Choose a random $b\gets\{0,1\}$.
\item Set $b'\gets\calA^{{\sf LR}^b_{\com}(\cdot,\cdot)}(1^n)$.
\item Output 1 iff $b'=b$.
\end{enumerate}
\end{description}
\end{description}
\begin{theorem}
If $\com$ is a {\it non-interactive perfectly-binding commitment scheme}, then for every non-uniform PPT adversary $\calA$ there exists a negligible function $\mu(\cdot)$ such that
$$\Pr[\text{\sf LR-commit}_{\com,\calA}(1^n)=1]\leq\frac{1}{2}+\mu(n).$$
\end{theorem}
\subsection{Non-Constant Round Zero Knowledge}

\begin{adjustbox}{minipage=1\linewidth,fbox}
\begin{protocol}[Zero-Knowledge Proof for 3-Coloring]$ $
    \begin{description}
        \item[Common input.] a graph $G=(V,E)$ with $V=\{v_1,\cdots,v_n\}$
        \item[Auxiliary input for $P$.] a coloring of the graph $\psi:V\to\{1,2,3\}$ such that 
        
        for every $(v_i,v_j)\in E$ it holds that $\psi(v_i)\neq\psi(v_j)$
        \item[The proof system:] Repeating the following $n\cdot\abs{E}$ times:
        $ $\newline\newline
        \begin{adjustbox}{minipage=0.6\linewidth}
            \begin{tabular}{lcl}
            $P(G,\psi;r_P)$ & & $V(G;r_V)$ \\
            \\
            Select a random permutation \\
            $\pi$ over $\{1,2,3\}$, define \\
            $\phi(v)=\pi(\psi(v))$ for all $v\in V$.\\
            Computes $c_i=\com(\phi(v_i))$ \\
            for all $i\in[n]$ & \rextlinearrow{(c_1,\cdots,c_n)}{24} & \\
            & & $e\in_R E$\\
            & \lextlinearrow{e}{24} &\\ 
            Let $e=(v_i,v_j)$, send \\
            & \rextlinearrow{\decom(c_i),\decom(c_j)}{24} & \\
            & & Check $\phi(v_i)$ and $\phi(v_j)$ \\
            & & is valid and $\phi(v_i)\neq\phi(v_j)$.\\
            & & if not, output 0. \\
            \end{tabular}
        \end{adjustbox}
        \\ \\
        if all checks pass in all iterations, the verifier outputs 1.
    \end{description}
    \label{proto:zk-3-color-nonconstant}
\end{protocol}
\end{adjustbox}
\\ \\
Regarding the {\it soundness} of the proof system. In each iteration, the cheating prover can only success if it guess the query of honest verifier, the probability of guessing right is $\frac{1}{\abs{E}}$.
\subsubsection{The Rewinding Technique and Formal Proof of Security}
We begin by giving an intuition of the simulator process when we model the commitments as perfect envelopes that reveal nothing until opened (in constrast to our real $\com$, which is computationally-hiding). 
\begin{enumerate}
\item The simulator first guess a random edge $e=(v_i,v_j)\in_R E$ with the hope that the verifier will query that edge. 
\item The simulator then sends the verifier commitments to a coloring such that $v_i$ and $v_j$ is given different colors in $\{1,2,3\}$ and the rest is set to zero.
\item If the verifier replies with the edge $e'=e$, the simulator opens the envelopes of $v_i$ and $v_j$ and the simulation of this iteration is complete.
\item Otherwise the simulator rewinds the verifier to the beginning of the iteration and tries again with fresh randomness.
\end{enumerate}
\begin{description}[leftmargin=0cm]
\item[On dealing with aborts.] One strategy is to state that if the verifier returns an illegal value, then the honest prover halts. However, in this case, it is easier for our proof of security to state that the honest prover interpret any invalid reply as a default edge.
\end{description}
The expected number of repetitions in one iteration is $\abs{E}$, and probability that more than $n\cdot\abs{E}$ repetitions are needed is negligible. 
\begin{remark} How is rewinding possible? And why does the existence of simulator not contradict soundness?
\end{remark}

\begin{algorithm}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
$\simu^{V^*(x,z,r,\cdot)}(G;r)$\;
\Input{$G=(V,E)$, with $V=\{v_1,\cdots,v_n\}$}
\Output{A simulated view close to real view of $P_2$.}
Initializes the message history transcript $\vec{m}\gets\lambda$\;
\Repeat{$n\cdot\abs{E}$ times}{
    \For{$j=1,\cdots,n\cdot\abs{E}$}{
        $(v_k,v_l)\in_R E$\;
        Choose $\phi(v)$ such that $\phi(v_k)\in_R\{1,2,3\}$ and $\phi(v_l)\in_R\{1,2,3\}\backslash\{\phi(v_k)\}$. For all other $v_i\in V\backslash\{v_k,v_l\},\phi(v_i)=0$\;
        \For{$i=1,\cdots,n$}{$c_i=\com(\phi(v_i))$}
        "Send" $(c_1,\cdots,c_n)$ to $V^*$. and Let $e\in E$ be the reply\;
        \If{$e=(v_k,v_l)$}{
            Update the message history transcript as
            $\vec{m}\gets(\vec{m},(c_1,\cdots,c_n),(\decom(c_k),\decom(c_l)))$\;
            $\bf break$
        }
        \Else{
            "rewind" the verifier to the beginning of for loop\;
        }
    }
    Output $\perp$
}
Output $\vec{m}$\;

\caption{Simulator of Protocol \ref{proto:zk-3-color-nonconstant}.} \label{alg:zk-3-color-nonconstant-sim}
\end{algorithm}

\insertbibliography{crypto}
\nocite{*} 