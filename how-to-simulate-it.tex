\course{Complexity Reading Group}{"How to Simulate It" Notes}{Kai-Ming, Chung}{Yin-Hsun, Huang}
\vspace*{4mm} 

\section{Introduction}
Simulation is a way of comparig what happens in the "real world" to what happens in an "ideal world" where the primitive in question is {\it secure by definition}. We start our discussion from semantic security of an private-key encryption. And move on to two-party computation. The semi-honest adversary model is relativly easy and is discussed in Section 4. The rest of article focus on the malicious adversary model.

\vspace{4mm}
\begin{tabular}{|l|c|c|}
\hline
Functionality & Input & Output\\
\hline
(Sec. 5) Zero Knowledge & X & X\\
(Sec. 6) Coin Tossing & X & O \\
(Sec. 7) Oblivious Transfer & O & O \\
\hline
\end{tabular}

\section{Preliminaries and Notation}
\YHnote{Lindel made some discussion on the order of quantifiers for computational indistinguishability. I should elaborate it.}

\section{Semantic Security}
We begin by recalling the definition of semantic security.
\begin{definition}[Semantic Security] A {\it private-key encryption scheme} $(G,E,D)$ is {\bf semantically secure} (in the private-key model) if for every non-uniform probabilistic-polynomial time algorithm $\calA$, there exists a non-uniform probabilistic-polynomial time algorithm $\calA'$ such that for every probability ensemble $\{X_n\}_{n\in\N}$ with $\abs{X_n}\leq poly(n)$, every pair of polynomially-bounded functions $f,h:\{0,1\}^*\to\{0,1\}^*$, every positive polynomial $p(\cdot)$ and all sufficiently large $n$:
\begin{align*}
    \Pr[k\leftarrow G(1^n);\calA(1^n,E_k(X_n), h(1^n,X_n)) = f(1^n, X_n)] \\
    < \Pr[\calA'(1^n,1^{\abs{X_n}}, h(1^n,X_n)) = f(1^n,X_n) ]+ \frac{1}{p(n)}
\end{align*}
with probability over $X_n$ and the randomness of $(G,E)$ and $\calA$ or$\calA'$.
\end{definition}
Though the definition of {\it semantic security} is not directly related to simulation-based paradigm. In particular, $\calA'$ simulate the execution of $\calA$ as follows:
\begin{description}
    \item [\bf Simulator $\calA'$:] Upon input $1^n,h=h(1^n,X_n)$, algorithm $\calA'$ works as follows
    \begin{enumerate}
        \item $\calA'$ runs $G(1^n)$ in order to receive $k$.
        \item $\calA'$ computes $c=E_k(0^{\abs{X_n}})$
        \item $\calA'$ outputs $\calA(1^n,c,a^{\abs{X_n}},h)$
    \end{enumerate}
\end{description}
If encryptions are {\it indistinguishable}, then the simulation process should output the same as $\calA$ with approximately the same probability.
\section{Secure Computation - Semi-Honest Adversaries}
\subsection{Background}
The {\it semi-honest adversaries} in two-party computation is an adversary that follows the protocol but wants to learn information from the transcript of messages. Hence the nickname, honest-but-curious adversaries. 
\begin{remark}
    This is a very weak adversary model. There are no guarantees to the security against adversaries that deviate from the protocol.
\end{remark}

\begin{definition}[Two-party computation] A two-party random process maps pairs of inputs to pairs of outputs. We refer to such a process as a {\bf functionality} and denote it $f:\{0,1\}^*\times\{0,1\}^*\to\{0,1\}^*\times\{0,1\}^*$, where $f=(f_1,f_2)$. For every pair of inputs $x,y\in\{0,1\}^n$, the pair of output is a random variable $(f_1(x,y),f_2(x,y))$. The first (resp. second) party (with input $x$ (resp. $y$)) wishes to obtain $f_1(x,y)$ (resp. $f_2(x,y)$).
\end{definition}
The goal of simulator is to generate the view only from a party's input and output. The security here is formalized by saying that a party's view in a protocol execution be simulatable given only its input and output. 
\begin{description}
\item[Definition of security.] We begin with the following notation:
\begin{itemize}
    \item Let $f=(f_1,f_2)$ be a PPT functionality and let $\pi$ be a two-party protocol for computing $f$.
    \item The view of the $i$th, where $i=1$ (resp. $i=2$), party is ${\sf view}^\pi_i(x,y,n)$ and equals $(w,r^i;m^i_1,\cdots,m^i_t)$, where $w=x$ (resp. $w=y$). $r^i$ is the internal randomness of $i$th party and $m^i_\ell$ is the $\ell$th message that $i$th party received.
    \item The output of the $i$th party is denoted by $\out^\pi_i(x,y,n)$. We denote the joint output of both parties by $\out^\pi(x,y,n)=(\out^\pi_1(x,y,n))(\out^\pi_2(x,y,n))$.
\end{itemize}
\end{description}
\begin{definition}Let $f=(f_1,f_2)$ be a functionality. We say that {\sf$\pi$ securely computes $f$ in the presence of static semi-honest adversarits} if there exists PPT algorithm $\calS_1$ and $\calS_2$ such that
\begin{align*}
    \{(\calS_1(1^n,x,f_1(x,y)), f(x,y))\}_{x,y,n} \stackrel{c}{\equiv} \{(\view^\pi_1(x,y,n),\out^\pi(x,y,n))\}_{x,y,n} \\
    \{(\calS_2(1^n,x,f_2(x,y)), f(x,y))\}_{x,y,n} \stackrel{c}{\equiv} \{(\view^\pi_2(x,y,n),\out^\pi(x,y,n))\}_{x,y,n}
\end{align*}
where $x,y\in\{0,1\}^*$ such that $\abs{x} = \abs{y}$, and $n\in\N$.
\end{definition}
Observe that for probabilistic functionalities, the simulator needs to generate a joint distribution of $(\view_i^\pi(x,y), \out^\pi(x,y))$. We can obtain a simpler security formulation for deterministic functionalities:
\begin{description}
\item[Correctness,] meaning that the output of the parties is correct. Formally, it requires that there exists a negligible function $\mu$ such that for every $x,y\in\{0,1\}^*$ and every $n$
    $$\Pr[\out^\pi(x,y,n)\neq f(x,y,n)]\leq\mu(n)$$
\item[Privacy] requires that there exists PPT $\calS_1$ and $\calS_2$ such that
\begin{align*}
    \{\calS_1(1^n,x,f_1(x,y))\}_{x,y,n} \stackrel{c}{\equiv} \{\view^\pi_1(x,y,n)\}_{x,y,n} \\
    \{\calS_2(1^n,y,f_2(x,y))\}_{x,y,n} \stackrel{c}{\equiv} \{\view^\pi_2(x,y,n)\}_{x,y,n} \\
\end{align*}
\end{description}
\begin{remark} There are many problemss that become trivial in the case of semi-honest adversaries. Such problems include zero knowledge, commitment and coin-tossing.
\end{remark}
\subsection{Oblivious Transfer for Semi-Honest Adversaries}
The bit oblivious transfer functionality is defined by $f((b_0,b_1), \sigma )=(\lambda,b_\sigma)$, where $b_0,b_1\in\{0,1\}$ and $\lambda$ denotes empty string.
\subsubsection{Background and Tools}
\begin{definition}[Trapdoor Permutations]
Trapdoor permutations is a collection of functions $\{f_\alpha\}_\alpha$ accompanied by four PPT algorithms $I,S,F,F^{-1}$ such that:
\begin{enumerate}
\item $I(1^n)$ select a random $n$-bit index $\alpha$ of a premutation $f_\alpha$ and a corresponding trapdoor $\tau$. Let $I_1(1^n)\equiv\alpha$.
\item $S(\alpha;r)$ samples an (almost uniform) element in the domain of $f_\alpha$. We assume $r\in\{0,1\}^n$.
\item $F(\alpha,x)=f_\alpha(x)$
\item $F^{-1}(\tau,y)=f^{-1}_\alpha(y)$
\end{enumerate}
\end{definition}
\YHnote{Need to check the security definition of T.P.}

\begin{definition}[Enhanced Trapdoor Permutations] A collection of {\sf trapdoor permutations} is a collection of {\sf enhanced trapdoor permutations} if for every non-uniform PPT adversary $\calA$ there exists a negligible function $\mu(\cdot)$ such that for every $n$,
$$\Pr[\calA(1^n,\alpha,r)=f^{-1}(S(\alpha;r))]\leq\mu(n)$$
where $\alpha\leftarrow I_1(1^n)$ and $r\stackrel{R}{\leftarrow}\{0,1\}^n$.
\end{definition}
\begin{definition}[Hard-Core Predicate] We say that $B$ is a {\sf hard-core predicate} of $(I,S,F,F^{-1})$ if for every non-uniform PPT adversary $\calA$ there exists a negligible function $\mu(\cdot)$ such that for every $n$,
$$\Pr[\calA(1^n,\alpha,r)=B(\alpha, f^{-1}(S(\alpha;r)))]\leq\frac{1}{2}+\mu(n)$$
\end{definition}
\begin{adjustbox}{minipage=\linewidth,fbox}
\begin{protocol}[Bit Oblivious Transfer]$ $
    \begin{description}
        \item[Inputs.] $P_1$ has $(b_0,b_1)$ and $P_2$ has $\sigma\in\{0,1\}$. Both have $(I,S,F,F^{-1})$ and corresponding hard-core predicate $B$.
        \item[The Protocol.] $ $\newline\newline
        \begin{adjustbox}{minipage=0.6\linewidth}
            \begin{tabular}{lcc}
            $P_1((b_0,b_1);r_1)$ & & $P_2(\sigma;(r_0^2,r_1^2))$ \\
            \\
            $(\alpha,\tau)\gets I(1^n;r_1)$ & \rextlinearrow{\alpha}{24} & \\
            & & $x_\sigma\gets S(\alpha;r_\sigma^2)$\\
            & & $y_{1-\sigma}\gets S(\alpha;r_{1-\sigma}^2)$\\
            & & compute $y_\sigma = F(\alpha,x_\sigma) $\\
            & \lextlinearrow{y_0,y_1}{24} & \\
            compute $x_0=F^{-1}(\tau,y_0)$ & & \\
            $x_1=F^{-1}(\tau,y_1)$ & & \\
            and \\ \\
            $\beta_0=B(\alpha,x_0)\oplus b_0$ & \rextlinearrow{\beta_0,\beta_1}{24}\\
            $\beta_1=B(\alpha,x_1)\oplus b_1$ & & compute $b_\sigma=B(\alpha,x_\sigma)\oplus\beta_\sigma$ \\
            \end{tabular}
        \end{adjustbox}
    \end{description}
\end{protocol}
\end{adjustbox}
\vspace{2mm}

\begin{algorithm}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
$S_1(1^n,(b_0,b_1);r,r_0,r_1)$\;
\Input{The input of $P_1$ and the ideal functionality output $f_1(\cdot,\cdot)$.}
\Output{A simulated view close to real view of $P_1$.}
\caption{Simulator of corrupted $P_1$.} \label{alg:bot-p1-sim}
Initialize $P_1$ with random tape $r$\;
Computes $(\alpha,\tau)\gets I(1^n;r)$\;
Computes $y_0\gets S(\alpha;r_0), y_1\gets S(\alpha;r_1)$\;
Output $((b_0,b_1),r;(y_0,y_1))$\;
\end{algorithm}

\begin{algorithm}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
$S_2(1^n,\sigma,b_\sigma,;r,r_0,r_1)$\;
\Input{The input of $P_2$ and the ideal functionality output $f_2(\cdot,\cdot)$.}
\Output{A simulated view close to real view of $P_2$.}
Initialize $P_2$ with random tape $(r_0,r_1)$\;
Computes $(\alpha,\tau)\gets I(1^n;r)$\;
Computes $x_\sigma\gets S(\alpha;r_\sigma), y_{1-\sigma}\gets S(\alpha;r_{1-\sigma})$\;
Set $x_{1-\sigma}=F^{-1}(\tau,y_{1-\sigma})$\;
Compute $\beta_\sigma=B(\alpha,x_\sigma)\oplus b_\sigma$\;
$\beta_{1-\sigma}=B(\alpha,x_{1-\sigma})$\;
Output $(\sigma,r_0,r_1;\alpha,(\beta_0,\beta_1))$\;

\caption{Simulator of corrupted $P_2$.} \label{alg:bot-p2-sim}
\end{algorithm}

\begin{description}
\item[$P_1$ is currupted.] Construct simulator $S_1$ as Algorithm \ref{alg:bot-p1-sim}. \\
We argue that the output of $S_1$ is statistically close to the view of corrupted $P_1$ with random tape equals to $r$. The simulator has no information about $P_2$'s input $\sigma$, so it cannot compute $y_\sigma=F(\alpha,x_\sigma)$. However, the definition of trapdoor permutation states that the output of $S(\alpha)$ is almost uniform. Thus,
    $$\{(F(\alpha,x_0),y_1)\}\stackrel{s}{\equiv}\{(y_0,y_1)\}\stackrel{s}{\equiv}\{(y_0,F(\alpha,x_1))\}$$
where $x_0,x_1,y_0,y_1$ are sampled by $S(\alpha)$ with independent randomness. This implies
$$\{S_1(1^n,(b_0,b_1))\}\sequiv\{\view^\pi_1((b_0,b_1),\sigma)\}$$
\begin{remark}
Step 1. of the simulator is necessary since it binds the view of simulator with the real view of $P_1$.
\end{remark}
\item[$P_2$ is corrupted.] Construct simulator $S_2$ as Algorithm \ref{alg:bot-p2-sim}:

\end{description}
\insertbibliography{crypto} 
\nocite{*} 